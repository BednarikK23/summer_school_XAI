{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Scaling the Patch Camelyon\n",
    "\n",
    "Welcome to the second part of the Patch Camelyon project! In this section, we‚Äôll take your deep learning pipeline to the next level by focusing on scalability and maintainability. We‚Äôll leverage PyTorch Lightning and Hydra to build a robust pipeline, integrate logging for comprehensive monitoring, and set up job submission for efficient training on a cluster.\n",
    "\n",
    "\n",
    "### Notebook Structure üìö\n",
    "\n",
    "1. Drawbacks of the Current Approach üìâ\n",
    "2. Configuring with Hydra ‚öôÔ∏è\n",
    "3. Creating a Proper ML Project with Version Control üì¶\n",
    "4. Integrating Logging with MLFlow üìä\n",
    "5. Job Submission for Scalable Training üñ•Ô∏è\n",
    "\n",
    "**Authors:**<br>\n",
    "&copy; 2024; Matƒõj Pek√°r, V√≠t Musil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install hydra-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Drawbacks of the Current Approach üìâ\n",
    "\n",
    "While developing in a single notebook is convenient for quick experimentation, it has its limitations, especially as your project grows. Let‚Äôs dive into some of the key challenges we face with this approach:\n",
    "\n",
    "### Scalability Challenges\n",
    "\n",
    "- **Resource Management**: Our current setup doesn‚Äôt automatically free up resources after job completes, leading to potential wastage.\n",
    "\n",
    "- **Long-Running Jobs**: There‚Äôs no **checkpointing**, meaning if a long training job gets interrupted, we lose all progress. This makes it difficult to handle more complex models or larger datasets efficiently.\n",
    "\n",
    "- **Distributed Training**: Our current setup doesn‚Äôt support distributed training, which is essential for leveraging multiple GPUs or nodes to speed up training on large datasets.\n",
    "\n",
    "\n",
    "### Traceability Issues\n",
    "\n",
    "- **Parameter Tracking**: It‚Äôs challenging to track which parameters we‚Äôve tried, making it difficult to understand what worked and what didn‚Äôt. We lack a clear history of our experiments, leading to potential redundancy or missed insights.\n",
    "\n",
    "- **Results Comparison**: Comparing results from different runs is cumbersome without a systematic way to log and retrieve this information.\n",
    "\n",
    "\n",
    "### Reproducibility Challenges\n",
    "\n",
    "- **Version Control**: Without proper version control, it‚Äôs hard to ensure that the code we‚Äôre running today is the same as what we‚Äôll run tomorrow. This makes it difficult to share our work or reproduce results later.\n",
    "\n",
    "- **Randomness Control**: We haven‚Äôt set a seed for random operations, which means our results might vary from run to run, making it hard to trust the outcomes.\n",
    "\n",
    "    > **Note**: Not all [algorithms](https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms) in PyTorch are deterministic, but setting a seed can help ensure reproducibility to some extent.\n",
    "\n",
    "\n",
    "\n",
    "By highlighting these drawbacks, we can see why it‚Äôs important to transition to a more scalable, traceable, and reproducible approach. This sets the stage for the next steps in our project, where we‚Äôll address some of these issues and build a more robust machine-learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring with [Hydra](https://hydra.cc) ‚öôÔ∏è\n",
    "\n",
    "Hydra is a powerful configuration management tool that makes it easy to compose and override configurations, offering a clean and maintainable approach to setting up your deep learning pipeline. By separating configuration from code, Hydra allows you to store settings in YAML files, making it simple to experiment with different hyperparameters and configurations. Instead of hard-coding parameters, you can adjust them in a YAML file, making your pipeline more flexible and easier to manage.\n",
    "\n",
    "Hydra is built on top of OmegaConf so if you are looking for a more detailed explanation of the configuration system, you can check out the [OmegaConf documentation](https://omegaconf.readthedocs.io/en/2.3_branch/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with Hydra\n",
    "\n",
    "Before we dive into using Hydra, let's get the necessary configuration files in place. Run the following cell to download and store them in the `hydra_configs` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get(\n",
    "    \"https://youtrack.rationai.cloud.e-infra.cz/api/files/225-202?sign=MTcyNTU4MDgwMDAwMHwxLTh8MjI1LTIwMnx2UnZDMksyM1dQNExDTHlTdDhxbV9nOGZVY2tBS0lHMEtzTkFBRjlBUk1VDQo&updated=1725401287969&forceDownload=true\"\n",
    ")\n",
    "with ZipFile(BytesIO(r.content)) as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, we‚Äôll show you the basics of using Hydra. For the exercises ahead, you‚Äôll work with YAML files stored in the `hydra_configs` folder. If you get stuck use the documentation or ask ChatGPT for help.\n",
    "\n",
    "But before we dive in, we‚Äôll define a simple helper method. Since we‚Äôre working in a Jupyter notebook environment, this method will simulate running the code from the command line with Hydra, allowing you to see how it all fits together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "\n",
    "def parse_hydra_conf(\n",
    "    config_name: str,\n",
    "    config_path: str = \"hydra_configs\",\n",
    "    cli_args: list[str] | None = None,\n",
    ") -> DictConfig:\n",
    "    with hydra.initialize(version_base=None, config_path=config_path):\n",
    "        return hydra.compose(config_name=config_name, overrides=cli_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "code {\n",
    "    color: purple;\n",
    "    background-color: #f0f0f0;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"alert alert-success code\">\n",
    "    <h3>TASK 2.1: Defining A List and Dictionary</h3>\n",
    "    <p>\n",
    "        In this task, you‚Äôll be working with the <code>hydra_configs/task1.yaml</code> file. Your goal is to define: <ul> <li>A list</li> <li>A dictionary</li> <li>A list of dictionaries</li> </ul> Ensure that everything is set up correctly so that the assertions run without any errors.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "config = parse_hydra_conf(\"task1.yaml\")\n",
    "print(OmegaConf.to_yaml(config))\n",
    "\n",
    "assert config.learning_rates == [1e-3, 1e-4, 1e-5]\n",
    "assert config.loss_weights == {\"dice\": 1.0, \"ce\": 0.9}\n",
    "assert config.optimizers == [\n",
    "    {\"name\": \"sgd\", \"learning_rate\": 1e-4},\n",
    "    {\"name\": \"adam\", \"learning_rate\": 1e-3, \"decay\": 1e-4},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "code {\n",
    "    color: purple;\n",
    "    background-color: #f0f0f0;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"alert alert-success code\">\n",
    "    <h3>TASK 2.2: Changing, Overriding and Deleting Configuration</h3>\n",
    "    <p>\n",
    "        In this task, you‚Äôll be working with the <code>hydra_configs/task2.yaml</code> file. Your objective is to override specific configuration settings directly from the command line (by defining the <code>cli_args</code> variable in this notebook). <br><br> Specifically, you‚Äôll need to: <ul> <li>Set the <code>checkpoint</code> parameter to <code>None</code> (you can do this one directly inside the YAML file) .</li> <li>Set <code>trainer.max_epochs</code> to <strong>10</strong>.</li> <li>Add <code>data.batch_size</code> with a value of <strong>32</strong>.</li>\n",
    "        <li>Delete the <code>trainer.logger</code>.</li> </ul> This task will help you understand how to dynamically adjust configurations using Hydra.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_args: list[str] = [\n",
    "    \"checkpoint=null\",\n",
    "    \"trainer.max_epochs=10\",\n",
    "    \"+data.batch_size=32\",\n",
    "    \"~trainer.logger\",\n",
    "]\n",
    "\n",
    "config = parse_hydra_conf(\"task2.yaml\", cli_args=cli_args)\n",
    "print(OmegaConf.to_yaml(config))\n",
    "\n",
    "assert config.checkpoint is None\n",
    "assert config.trainer.max_epochs == 10\n",
    "assert config.data.batch_size == 32\n",
    "assert not hasattr(config.trainer, \"logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "code {\n",
    "    color: purple;\n",
    "    background-color: #f0f0f0;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"alert alert-success code\">\n",
    "    <h3>TASK 2.3: Interpolation</h3>\n",
    "    <p>\n",
    "        In this task, you‚Äôll be working with the <code>hydra_configs/task3.yaml</code> file, where you'll leverage interpolation to avoid redundancy and keep your configuration clean and maintainable. Complete the TODOs in the YAML file to interpolate the values correctly.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parse_hydra_conf(\"task3.yaml\")\n",
    "config.batch_size = 64\n",
    "config.data.val.transforms = config.data.val.transforms[:-1]\n",
    "print(OmegaConf.to_yaml(config))\n",
    "\n",
    "\n",
    "assert config.data.test.transforms == [\"rotate\", \"flip\"]\n",
    "assert config.data.batches == 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "code {\n",
    "    color: purple;\n",
    "    background-color: #f0f0f0;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"alert alert-success code\">\n",
    "    <h3>TASK 2.4: Instantiation</h3>\n",
    "    <p>\n",
    "        In this task, you‚Äôll be modifying the <code>hydra_configs/task4/default.yaml</code> file and use the <code>hydra_configs/task4/example.py</code>. Your objective is to define define model named \"unet\" with backbone which has 3 classes. We encourage you to also use the interpolation from the previos task. This task will help you understand how you can instantiate classes and functions directly from config files. This is useful when you want to define your model, dataset, or any other object in a configuration file.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra_configs.task4.example import Backbone, Model\n",
    "\n",
    "\n",
    "config = parse_hydra_conf(\"default.yaml\", config_path=\"hydra_configs/task4\")\n",
    "print(OmegaConf.to_yaml(config))\n",
    "\n",
    "model = hydra.utils.instantiate(config.class_example)\n",
    "function = hydra.utils.instantiate(config.function_example)\n",
    "\n",
    "assert isinstance(model, Model)\n",
    "assert model.name == \"unet\"\n",
    "assert isinstance(model.backbone, Backbone)\n",
    "assert model.backbone.num_classes == 3\n",
    "\n",
    "\n",
    "assert isinstance(function, Model)\n",
    "assert function.name == \"unet\"\n",
    "assert isinstance(function.backbone, Backbone)\n",
    "assert function.backbone.num_classes == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "code {\n",
    "    color: purple;\n",
    "    background-color: #f0f0f0;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"alert alert-success code\">\n",
    "    <h3>TASK 2.5: Config Groups</h3>\n",
    "    <p>\n",
    "        In this task, you‚Äôll be editing only the <code>hydra_configs/task5/default.yaml</code> file. The goal is to streamline your configurations by organizing them into groups. Here‚Äôs what you need to do: <ul> <li>Start with the <code>base.yaml</code> as your foundation.</li> <li>Link the <code>/data/datasets/cityscapes.yaml</code> configuration to <code>data.train</code>.</li> <li>Set the <code>data.train.transforms</code> to use the transformations defined in <code>/data/transforms/train.yaml</code>.</li> <li>Override the <code>data.train.path</code> to point to <code>\"/some/data\"</code>.</li> <li>Finally, make the <code>model.backbone</code> required.</li> </ul> This task will help you understand how to group configurations effectively and set up defaults that make your pipeline more flexible and easier to manage. By organizing your configs in this way, you‚Äôre setting yourself up for smoother and more controlled experimentation. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_args: list[str] = [\"model/backbone=vgg16\"]\n",
    "\n",
    "config = parse_hydra_conf(\n",
    "    \"default.yaml\", config_path=\"hydra_configs/task5\", cli_args=cli_args\n",
    ")\n",
    "print(OmegaConf.to_yaml(config))\n",
    "\n",
    "assert config.data.batch_size == 64\n",
    "assert config.model.backbone.name == \"vgg16\"\n",
    "\n",
    "assert config.data.train.type == \"cityscapes\"\n",
    "assert config.data.train.path == \"/some/data\"\n",
    "assert config.data.train.transforms == [\"rotate\", \"flip\", \"normalize\"]\n",
    "\n",
    "\n",
    "cli_args: list[str] = [\"model/backbone=resnet18\"]\n",
    "\n",
    "config = parse_hydra_conf(\n",
    "    \"default.yaml\", config_path=\"hydra_configs/task5\", cli_args=cli_args\n",
    ")\n",
    "print(OmegaConf.to_yaml(config))\n",
    "\n",
    "assert config.model.backbone.name == \"resnet18\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Proper ML Project with Version Control üì¶\n",
    "\n",
    "Let's dive into the next step of professionalizing your machine learning workflow! In this section, you'll set up a robust version control system to keep your project organized, reproducible, and collaborative.\n",
    "\n",
    "\n",
    "### Step-by-Step Guide\n",
    "1. **Get Started with Version Control**:\n",
    "\n",
    "    First, create an account on a version control platform like GitHub, GitLab, or Bitbucket. This is where you‚Äôll manage your project and track its history.\n",
    "\n",
    "2. **Set Up Your Project**:\n",
    "\n",
    "    Create a new **PUBLIC** repository by either:\n",
    "    - Directly importing our [template repository](https://gitlab.ics.muni.cz/rationai/digital-pathology/templates/machine-learning.git)\n",
    "    - Or by cloning it to your local machine and pushing it to your repository.\n",
    "\n",
    "3. **Customize the Template**:\n",
    "\n",
    "    Now, it‚Äôs time to fill in the template with what you‚Äôve learned so far. Aim to extract all configurable elements into separate configuration files for better flexibility. Keep your configurations simple and intuitive‚Äîavoid overcomplicating things!\n",
    "    \n",
    "    > ‚ö†Ô∏è Tip: The image you pulled includes a sample dataset to test your pipeline. You‚Äôll find it at `/mnt/data/PatchCamelyon`. This path is consistent with the cluster environment, so no need to worry about data paths.\n",
    "\n",
    "\n",
    "### Dependency Management Made Easy with [PDM](https://pdm-project.org/en/latest/)\n",
    "\n",
    "We‚Äôre using PDM for dependency management. Unlike pip, PDM offers a lock file, ensuring stable and reproducible development. Plus, it‚Äôs more flexible than Poetry, allowing you to run custom scripts easily.\n",
    "\n",
    "Here are some handy commands to get you started:\n",
    "```bash\n",
    "pdm install # install dependencies\n",
    "pdm add <package> # add a new package\n",
    "pdm remove <package> # remove a package\n",
    "pdm run <file> # run a file\n",
    "pdm <script> # run a script from pyproject.toml\n",
    "```\n",
    "\n",
    "### Running Your Pipeline\n",
    "\n",
    "To kick off your training, validation, testing, or prediction processes, simply use the following predifined scripts (defined in the `pyproject.toml`):\n",
    "```bash\n",
    "pdm train\n",
    "pdm val\n",
    "pdm test\n",
    "pdm predict\n",
    "```\n",
    ">**_NOTE:_** Before running the pipeline you have to export your `MLFLOW_TRACKING_USERNAME` and `MLFLOW_TRACKING_PASSWORD` as environment variables because logging to MLFlow is enabled by default.\n",
    "\n",
    "\n",
    "### What Your Project Should Look Like\n",
    "\n",
    "By the end of this task, your project structure should resemble this:\n",
    "```bash\n",
    ".\n",
    "‚îú‚îÄ‚îÄ LICENSE\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ configs\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ patch_camelyon\n",
    "‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ test.yaml\n",
    "‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ train.yaml\n",
    "‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ val.yaml\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transforms\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ test.yaml\n",
    "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ train.yaml\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ default.yaml\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ hydra\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ default.yaml\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ logger\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mlflow.yaml\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ model\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ backbone\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ patch_camelyon\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_module.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datasets\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ patch_camelyon.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ modeling\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backbone\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ binary_classifier.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ patch_camelyon_model.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ typing.py\n",
    "‚îú‚îÄ‚îÄ pdm.lock\n",
    "‚îî‚îÄ‚îÄ pyproject.toml\n",
    "```\n",
    "\n",
    "By the time you complete this task, you‚Äôll have a fully configured ML project that‚Äôs easy to manage, share, and extend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integrating Logging with [MLFlow](https://mlflow.org) üìä\n",
    "\n",
    "Now that we‚Äôve set up our project, it‚Äôs time to supercharge it with experiment tracking. We‚Äôll be using **MLFlow**, one of the most powerful open-source platforms for managing the end-to-end machine learning lifecycle.\n",
    "\n",
    "You can access our self hosted MLFlow instance at [https://biomedai-summer-mlflow.dyn.cloud.e-infra.cz](https://biomedai-summer-mlflow.dyn.cloud.e-infra.cz).\n",
    "\n",
    "### Step 1: Running Your First Logged Training Run\n",
    "\n",
    "Let‚Äôs kick things off by running a training script to see MLFlow in action. By default, your run should automatically log key elements like:\n",
    "\n",
    "- **Loss** and **metrics** during training.\n",
    "\n",
    "- **Git information** to track the code version.\n",
    "\n",
    "- **Console logs** for debugging and insights.\n",
    "\n",
    "\n",
    "> Tip: Run your training script a few times with different settings. This will allow you to explore the MLFlow UI, compare different runs, and visualize the results. It‚Äôs a great way to see how your model evolves with each experiment!\n",
    "\n",
    "\n",
    "### Step 2: Adding Checkpointing for Robust Training\n",
    "\n",
    "Imagine your training process gets interrupted‚Äîno one likes to start over from scratch, right? Let‚Äôs safeguard your progress by enabling **checkpointing**. This allows you to save your model‚Äôs state after each epoch so you can resume training if something goes wrong.\n",
    "\n",
    "Here‚Äôs how:\n",
    "\n",
    "1. **Add a Checkpoint Callback**:\n",
    "\n",
    "    Use the [ModelCheckpoint callback](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html) in your PyTorch Lightning `Trainer`. Configure it to track the **validation loss** and save checkpoints.\n",
    "    > **NOTE**: You should configure this entirely through Hydra.\n",
    "\n",
    "2. **Verify Checkpoint Saving**:\n",
    "\n",
    "    Run your training again and check if checkpoints are being saved as artifacts in MLFlow after each epoch. These will be super handy for resuming training or for later evaluation!\n",
    "\n",
    "\n",
    "### Step 3: Resuming Training from a Checkpoint\n",
    "\n",
    "To test out your checkpointing setup, try resuming your training from the last saved checkpoint:\n",
    "\n",
    "- Set the `checkpoint` parameter in your configuration to the MLFlow URI of the checkpoint you want to resume from.\n",
    "\n",
    "- Run your training script again and watch as it picks up right where it left off!\n",
    "\n",
    "\n",
    "### Step 4: Evaluating the Model on the Test Set\n",
    "\n",
    "Once you‚Äôve logged your checkpoints and explored how to resume from them, it‚Äôs time to evaluate your model:\n",
    "\n",
    "- Use the logged checkpoint to perform evaluation on your test set.\n",
    "\n",
    "- Check MLFlow to ensure that the results are logged properly.\n",
    "\n",
    "By integrating MLFlow into your pipeline, you‚Äôre not just training a model‚Äîyou‚Äôre creating a rich, trackable experiment history. This will help you and your team to easily reproduce results, compare different approaches, and ensure that you‚Äôre always moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Job Submission for Scalable Training üñ•Ô∏è\n",
    "\n",
    "To harness the full power of cluster computing, you'll need to submit your job to the cluster environment. But first, let's ensure your code is ready for prime time!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 1: Push Your Code to a Remote Repository\n",
    "\n",
    "\n",
    "Before you can submit your job, make sure your code is pushed to a **PUBLIC** remote repository (like GitHub, GitLab, or Bitbucket). This step is crucial for the accessibility of your code in a cluster environment, as well as the reproducibility of your experiments. By logging the commit hash, you can always trace back and reproduce the exact version of the code used in your experiment. This is a best practice in machine learning, helping ensure that your results are reliable and can be replicated.\n",
    "\n",
    "### Step 2: Download the Helm Chart\n",
    "\n",
    "Next, you'll need a [Helm chart](https://helm.sh/docs/topics/charts/) to submit your job to the cluster. Helm charts are like blueprints for deploying applications on Kubernetes, and they‚Äôll help you manage your training jobs efficiently.\n",
    "\n",
    "Run the following cell to download and store the Helm chart in the `job` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\n",
    "    \"https://youtrack.rationai.cloud.e-infra.cz/api/files/225-203?sign=MTcyNTU4MDgwMDAwMHwxLTh8MjI1LTIwM3wzNjJIZFpUY3JzLXU0R19fQ3pDOXBENlFJZ3NPb0h4NmlyMHQ1MDR3emU4DQo&updated=1725401404972&forceDownload=true\"\n",
    ")\n",
    "with ZipFile(BytesIO(r.content)) as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure Your Job\n",
    "\n",
    "Next, let‚Äôs set up your job configuration. This is where you'll specify your repository, the exact code version you want to run, and the commands for your training job.\n",
    "\n",
    "1. Open the `job/values.yaml` File. This is your job‚Äôs configuration file, where you‚Äôll set up the details needed to run your model on the cluster.\n",
    "\n",
    "2. Replace the value of `git.repository` field with the URL of your own repository.\n",
    "\n",
    "3. In the `git.commit` field, enter the specific commit hash or branch name you want the job to use.\n",
    "\n",
    "4. In the `script` field, specify the commands for your training job, like `pdm install` for dependencies and `pdm train` to start training.\n",
    "\n",
    "\n",
    "\n",
    "### Step 4: Submit Your Job\n",
    "\n",
    "Once your configuration is ready, it‚Äôs time to launch your training job on the cluster.\n",
    "\n",
    "Run the following command to submit your job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!helm install -f job/values.yaml job-<your_name> job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to cancel or delete your job, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!helm uninstall job-<your_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your job is submitted and resources become available, it will start running on the cluster. You can track its progress through the [MLFlow UI](https://biomedai-summer-mlflow.dyn.cloud.e-infra.cz)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
